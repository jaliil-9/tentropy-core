import { Challenge, Track } from '../types/challenge';

export const challenges: Challenge[] = [
    {
        "id": "redos-cpu-killer",
        "title": "The Regex Assassin",
        "difficulty": "Medium",
        "summary": "Fix a catastrophic regex backtracking vulnerability that freezes the production CPU.",
        "description": "\n# The Scenario\n\nYou are building a **data validation pipeline** that processes user-submitted strings before parsing them.\nTo prevent malicious input, you run a local Python validator to check patterns before further processing.\n\n# The Incident\n\nA user pasted a malformed pattern string into your API. Suddenly, your backend **froze**.\nNo errors, no crash logsâ€”just 100% CPU usage.\nYour downstream services are sitting idle, but your validation container is completely unresponsive.\n\n# The Stakes\n\n- **The Victim**: Your validation service.\n- **The Culprit**: A regex validator intended to check for repeating character patterns.\n- **The Result**: A \"ReDoS\" (Regular Expression Denial of Service) that forces your CPU to check millions of paths for a single string.\n\n# The Goal\n\nPatch the `validate_pattern` function.\n- The current logic uses `^((A+)+)+$` which has **O(2^n)** complexity.\n- **Your Task**: Rewrite the regex to accept valid patterns instantly (under 10ms) without hanging on malformed inputs.",
        "brokenCode": "import re\n\ndef validate_pattern(sequence):\n    # CONTEXT: We need to verify the input is a valid repeating pattern\n    # (a sequence consisting ONLY of the character 'A').\n    \n    # This pattern causes catastrophic backtracking.\n    # It attempts to match \"one or more As\", repeated, inside a repeating group.\n    pattern = r\"^((A+)+)+$\"\n    \n    if re.match(pattern, sequence):\n        return True\n    return False\n",
        "testCode": "import pytest\nimport multiprocessing\nimport time\nimport sys\n\ndef run_validation(sequence, return_dict):\n    try:\n        from solution import validate_pattern\n        start = time.time()\n        result = validate_pattern(sequence)\n        end = time.time()\n        return_dict['result'] = result\n        return_dict['duration'] = (end - start) * 1000 # ms\n    except Exception as e:\n        return_dict['error'] = str(e)\n\ndef test_catastrophic_backtracking():\n    \"\"\"\n    The Attack: 25 'A's followed by a Cytosine ('C').\n    The regex engine will try every combination of nested groups before failing.\n    We enforce a strict 1.0s timeout to simulate the 'Hang'.\n    \"\"\"\n    # 25 As followed by one C (which breaks the pattern)\n    attack_string = \"A\" * 25 + \"C\"\n    \n    manager = multiprocessing.Manager()\n    return_dict = manager.dict()\n    \n    p = multiprocessing.Process(target=run_validation, args=(attack_string, return_dict))\n    p.start()\n    \n    # Wait for 1.0 second max\n    p.join(timeout=1.0)\n    \n    if p.is_alive():\n        p.terminate()\n        p.join()\n        print(\"\\n[KERNEL] [1234.567890] watchdog: BUG: soft lockup - CPU#0 stuck for 22s! [python3:1337]\")\n        print(\"[WATCHDOG] Process stuck for 1000ms. Killing...\")\n        pytest.fail(\"âŒ CATASTROPHIC BACKTRACKING DETECTED: The pattern validator hung the CPU! (Timeout > 1.0s)\")\n    \n    if 'error' in return_dict:\n        pytest.fail(f\"Runtime Error: {return_dict['error']}\")\n        \n    duration = return_dict.get('duration', 0)\n    result = return_dict.get('result')\n    \n    # Success Criteria\n    assert result is False, \"The validator should reject the malformed sequence containing 'C'.\"\n    \n    # CALIBRATION: Measure baseline performance of the environment\n    start_cal = time.time()\n    for _ in range(100000): pass\n    end_cal = time.time()\n    calibration_time = (end_cal - start_cal) * 1000 # ms\n    \n    # Expected baseline on fast machine is ~3-5ms. If slower, scale the limit.\n    base_limit = 10.0\n    expected_calibration = 5.0 \n    \n    scaling_factor = max(1.0, calibration_time / expected_calibration)\n    adjusted_limit = base_limit * scaling_factor\n    \n    # Add a small buffer for noise\n    adjusted_limit += 5.0\n    \n    print(f\"\\n[DEBUG] Calibration: {calibration_time:.2f}ms | Scaling: {scaling_factor:.2f}x | Limit: {adjusted_limit:.2f}ms\")\n\n    if duration < adjusted_limit:\n        print(f\"[WAF] [ACCESS] 192.168.1.50 - - [02/Dec/2025:10:00:00 +0000] \\\"POST /api/validate HTTP/1.1\\\" 200 45 \\\"-\\\" \\\"Python/3.9\\\" (Time: {duration:.2f}ms)\")\n    else:\n        print(f\"[WAF] [WARN] Slow request detected: {duration:.2f}ms\")\n\n    assert duration < adjusted_limit, f\"Too slow! Took {duration:.2f}ms. Target < {adjusted_limit:.2f}ms (Adjusted for env speed).\"\n\ndef test_valid_input():\n    \"\"\"Ensure we didn't break valid pattern inputs.\"\"\"\n    from solution import validate_pattern\n    assert validate_pattern(\"AAAAAAAAAAAAAAAAAAAAAAAAA\") == True\n    assert validate_pattern(\"AAAAA\") == True\n",
        "successMessage": "Pattern Validated! Your validation service is now safe from ReDoS.",
        "debrief": "\n# Debrief: The ReDoS Trap in Input Validation\n\n## Why this matters\nYou were \"validating user input patterns\" before processing them.\nIn this challenge, we simulated a **repeating character validation**, but the principle applies to any pattern matching (e.g., email validation, phone numbers).\n\n## The Mechanics: O(2^n) Complexity\nThe regex `^((A+)+)+$` fails because of **Nested Quantifiers**.\nWhen the validator meets the character 'C' at the end of the string, it doesn't fail immediately. It backtracks and tries to regroup the previous 25 'A's in every possible combination (2^25 attempts) to see if *that* works.\n\n## The Impact on Production\nIn a production system:\n1.  **The Choke Point:** Regex validation runs *before* downstream processing.\n2.  **The Hang:** If this regex hangs, the application stops responding.\n3.  **The Cost:** Your hosting provider (AWS/GCP/Vercel) is still charging you for the CPU time while it spins at 100%.\n\n## Event Loop Blocking in Async Python\nEven if your application uses `async/await`, **regex operations are synchronous** and CPU-bound.\nWhen a ReDoS attack occurs:\n- The regex engine blocks the event loop\n- All other async tasks are starved\n- Your entire service becomes unresponsive, even for simple health checks\n\nTo prevent this in async systems:\n```python\n# Run CPU-intensive regex in a thread pool\nloop = asyncio.get_event_loop()\nresult = await loop.run_in_executor(None, validate_pattern, user_input)\n```\n\nOr better yet, use non-backtracking engines like `re2`.\n\n## The Fix\nSimplify.\n* **Bad:** `^((A+)+)+$`\n* **Good:** `^A+$`\n* **Better:** Use non-backtracking regex engines (like `re2`) or length limits for critical validation paths.\n"
    },
    {
        "id": "rate-limit-token-bucket-002",
        "title": "The Burst Handler",
        "difficulty": "Medium",
        "summary": "Implement a Token Bucket algorithm to allow traffic bursts while respecting long-term rate limits.",
        "description": "\n# The Scenario\n\nYou are scraping a strict API. It allows bursts (up to 10 requests instantly) but enforces a long-term rate of 1 request/second.\n\n# The Problem\n\nYour current implementation uses a `StrictLimiter`. It enforces a rigid 1-second gap between *every* request (Leaky Bucket style).\n\nWhile this is \"safe\" (you won't get banned), it is **too slow**. It fails to utilize the API's \"Burst\" allowance. If you have 10 requests to make, you should be able to make them *instantly* because you haven't used your allowance yet. The `StrictLimiter` forces you to wait 10 seconds.\n\n# The Goal\n\nRefactor the `StrictLimiter` class into a **Token Bucket** algorithm.\n- **Bank Tokens**: Accumulate tokens when idle (up to capacity 10).\n- **Burst**: Spend tokens instantly to make requests without waiting.\n- **Throttle**: Only wait when the bucket is empty.\n",
        "brokenCode": "import time\nimport requests\n\nclass StrictLimiter:\n    def __init__(self, rate):\n        self.rate = rate\n        # Strict spacing: 1 request every 1/rate seconds\n        self.interval = 1.0 / rate\n        self.last_call = 0.0\n\n    def wait(self):\n        # Good use of monotonic time, but bad logic for bursts!\n        now = time.monotonic()\n        elapsed = now - self.last_call\n        if elapsed < self.interval:\n            time.sleep(self.interval - elapsed)\n        self.last_call = time.monotonic()\n\n# TODO: Refactor this into a TokenBucket class!\n# class TokenBucket:\n#     def __init__(self, rate, capacity):\n#         ...\n#     def consume(self):\n#         ...\n\ndef fetch_data(url):\n    # This limiter is safe but too slow!\n    limiter = StrictLimiter(rate=1.0)\n    \n    # Simulate 10 requests\n    for _ in range(10):\n        limiter.wait()\n        response = requests.get(url)\n    \n    return \"Done\"\n",
        "testCode": "import pytest\nfrom unittest.mock import MagicMock, patch\nimport time\n\ndef test_burst_capability():\n    \"\"\"\n    Goal: Verify that the implementation allows a burst of 10 requests instantly.\n    The StrictLimiter fails this because it sleeps between every request.\n    \"\"\"\n    try:\n        from solution import TokenBucket\n    except ImportError:\n        pytest.fail(\"Could not import 'TokenBucket'. Did you refactor the class?\")\n\n    # We mock both time and monotonic to ensure the user can use either (though monotonic is preferred)\n    with patch('time.time') as mock_time, patch('time.monotonic') as mock_mono, patch('time.sleep') as mock_sleep:\n        # Shared time state\n        current_time = [1000.0] \n        \n        # Both clocks return the same flow of time for simplicity in tests\n        mock_time.side_effect = lambda: current_time[0]\n        mock_mono.side_effect = lambda: current_time[0]\n        \n        def sleep_side_effect(seconds):\n            current_time[0] += seconds\n        mock_sleep.side_effect = sleep_side_effect\n\n        # Initialize: Rate 1/sec, Capacity 10\n        bucket = TokenBucket(rate=1.0, capacity=10)\n\n        # BURST TEST: Consume 10 tokens\n        start_time = current_time[0]\n        for i in range(10):\n            bucket.consume()\n            print(f\"[GATEWAY] [INFO] Request forwarded to backend. TokenBucket: {10 - i - 1}/10. Latency: 0.1ms\")\n        end_time = current_time[0]\n\n        duration = end_time - start_time\n        \n        # StrictLimiter would take ~9-10 seconds here.\n        # TokenBucket should take 0 seconds (logic time).\n        assert duration < 0.1, f\"Too slow! 10 requests took {duration}s. Expected burst (~0s sleep).\"\n\ndef test_rate_limit_enforcement():\n    \"\"\"\n    Goal: Verify that after the burst, the rate limit (1/sec) is enforced.\n    \"\"\"\n    from solution import TokenBucket\n    \n    with patch('time.time') as mock_time, patch('time.monotonic') as mock_mono, patch('time.sleep') as mock_sleep:\n        current_time = [1000.0]\n        mock_time.side_effect = lambda: current_time[0]\n        mock_mono.side_effect = lambda: current_time[0]\n        \n        def sleep_side_effect(seconds):\n            current_time[0] += seconds\n        mock_sleep.side_effect = sleep_side_effect\n\n        bucket = TokenBucket(rate=1.0, capacity=10)\n\n        # Drain the bucket (10 tokens)\n        for _ in range(10):\n            bucket.consume()\n            \n        # Now bucket is empty. Next request MUST sleep ~1.0s\n        mock_sleep.reset_mock()\n        print(\"\\n[GATEWAY] [WARN] Rate Limit Exceeded for IP 10.0.0.1. Bucket Empty. Throttling...\")\n        bucket.consume()\n        \n        assert mock_sleep.called, \"You exceeded the rate limit! Should have slept when bucket was empty.\"\n        sleep_time = mock_sleep.call_args[0][0]\n        assert 0.9 <= sleep_time <= 1.1, f\"Should sleep ~1.0s, slept {sleep_time}s\"\n        print(f\"[GATEWAY] [INFO] Request allowed after throttle. Waited {sleep_time:.2f}s.\")\n",
        "successMessage": "Bucket Full! You've mastered the Token Bucket algorithm.",
        "debrief": "\n# Debrief: Traffic Shaping vs Rate Limiting\n\n## Mechanics\nThe Token Bucket algorithm is based on a simple refill formula:\n```python\n# Use monotonic time for durations!\nnew_tokens = (now - last_refill) * rate\n```\n\nWe cap the tokens at `capacity` so you can't accumulate infinite burst allowance.\n\n## Visualizing the Bucket\n```text\n       (Rate: 1 token/sec)\n              |\n              v\n      [  Token Drop  ]\n              |\n      +---------------+\n      |  T  T  T  T   |  <- Capacity (10)\n      |  T  T  T  T   |\n      +---------------+\n              |\n      [   Consumer    ]  <- Bursts drain tokens instantly\n              |\n              v\n       (Allowed Request)\n```\n\n## Traffic Shaping vs Rate Limiting\n\n### Rate Limiting (Token Bucket)\n- **Purpose**: Prevent abuse by limiting total throughput over time\n- **Behavior**: Allows bursts, then throttles\n- **Use Case**: API rate limits, scraping, preventing abuse\n- **Implementation**: Token Bucket (this challenge) or Sliding Window\n\n### Traffic Shaping (Leaky Bucket)\n- **Purpose**: Smooth out traffic to prevent jitter and maintain steady flow\n- **Behavior**: Rigid spacing between requests, no bursts\n- **Use Case**: Network QoS, video streaming, preventing congestion\n- **Implementation**: Leaky Bucket (strict spacing)\n\n**Key Difference**: Rate limiting cares about *total volume*. Traffic shaping cares about *consistent pacing*.\n\nIn production:\n- Use **Token Bucket** for user-facing APIs (allow bursts for good UX)\n- Use **Leaky Bucket** for backend-to-backend communication (prevent overwhelming downstream services)\n\n## Senior Engineer Notes\n> [!WARNING]\n> **Time is Hard**: Always use `time.monotonic()` for measuring durations. `time.time()` can jump backwards or forwards if the server syncs with NTP, which will break your rate limiter logic in production.\n\n> [!IMPORTANT]\n> **Distributed Systems**: This code only works for a single process. In a distributed system (e.g., Celery workers, Kubernetes), you must use a shared store like **Redis** (using `INCR` and `EXPIRE` or Lua scripts) to maintain the bucket state across all workers.\n"
    },
    {
        "id": "ai-cost-cache-002",
        "title": "The Wallet Burner",
        "difficulty": "Easy",
        "summary": "Slash API costs by 90% with exact match caching for duplicate queries.",
        "description": "\n# The Scenario\n\nYour AI-powered support chatbot is **burning $10,000/month** on OpenAI API calls.\n\nThe analytics reveal the truth:\n- **\"How do I reset my password?\"** â†’ asked 1,000 times/day\n- **\"What are your business hours?\"** â†’ asked 800 times/day\n- **\"Where's my order?\"** â†’ asked 600 times/day\n\nYou're paying OpenAI for **the same answer, over and over again**.\n\n# The Problem\n\nYour code makes a fresh API call every single time:\n\n```python\ndef get_answer(question):\n    return openai.chat.completions.create(...)  # $$$\n```\n\nEach call costs $0.03. With 10,000 requests/day for common questions, that's **$300/day** wasted.\n\n# The Goal\n\nImplement **Exact Match Caching** (Step 1 of production caching):\n1. Check if the question exists in a cache dictionary\n2. If found â†’ return cached answer (instant, free)\n3. If not â†’ call API, cache the result, return it\n\n**Requirements:**\n- Use a simple `dict` as cache (`{question: answer}`)\n- Only call the API when cache misses\n- The test will verify API is called only once for duplicate queries\n\n> **Note**: This is **exact string matching**. In production, you'd layer this with semantic matching (Step 2) for similar questions.\n",
        "brokenCode": "def get_answer(question):\n    \"\"\"\n    PROBLEM: Every call hits the API, even for duplicate questions.\n    This burns money and adds latency.\n    \"\"\"\n    # Simulate OpenAI API call\n    response = call_openai_api(question)\n    return response\n\ndef call_openai_api(question):\n    \"\"\"Mock API - in production this would be openai.chat.completions.create()\"\"\"\n    return f\"Answer to: {question}\"\n",
        "testCode": "import pytest\nfrom unittest.mock import patch, MagicMock\n\ndef test_cache_prevents_duplicate_api_calls():\n    \"\"\"\n    Verify that asking the same question twice only calls the API once.\n    \"\"\"\n    with patch('solution.call_openai_api') as mock_api:\n        mock_api.return_value = \"Cached answer\"\n        \n        from solution import get_answer\n        \n        question = \"How do I reset my password?\"\n        \n        # First call - should hit API\n        answer1 = get_answer(question)\n        print(f\"[OPENAI] [INFO] Outgoing Request: POST https://api.openai.com/v1/chat/completions (Tokens: 150)\")\n        print(f\"[BILLING] [INFO] Charge: $0.003\")\n        \n        assert mock_api.call_count == 1, \"First call should hit the API\"\n        assert answer1 == \"Cached answer\"\n        \n        # Second call - should use cache\n        answer2 = get_answer(question)\n        if mock_api.call_count == 1:\n            print(f\"[CACHE] [INFO] Cache HIT for key 'sha256:...' - Serving from Redis (0ms)\")\n        else:\n            print(f\"[OPENAI] [INFO] Outgoing Request: POST https://api.openai.com/v1/chat/completions (Tokens: 150)\")\n            print(f\"[BILLING] [INFO] Charge: $0.003 (DUPLICATE QUERY!)\")\n\n        assert mock_api.call_count == 1, f\"Cache miss! API was called {mock_api.call_count} times. Should be 1.\"\n        assert answer2 == \"Cached answer\"\n        \n        # Third call - still cached\n        answer3 = get_answer(question)\n        assert mock_api.call_count == 1, \"Cache still not working!\"\n\ndef test_different_questions_call_api():\n    \"\"\"\n    Verify that different questions aren't incorrectly cached together.\n    \"\"\"\n    with patch('solution.call_openai_api') as mock_api:\n        mock_api.side_effect = lambda q: f\"Answer to: {q}\"\n        \n        from solution import get_answer\n        \n        answer1 = get_answer(\"Question 1\")\n        answer2 = get_answer(\"Question 2\")\n        \n        # Should have called API twice for two different questions\n        assert mock_api.call_count == 2\n        assert answer1 == \"Answer to: Question 1\"\n        assert answer2 == \"Answer to: Question 2\"\n",
        "successMessage": "Cache Deployed! You've cut API costs by 90%.",
        "debrief": "\n# Debrief: The Cache Layer\n\n## Exact Match vs Semantic Caching\n\n**What you implemented:** Exact Match Caching\n- `\"How do I reset my password?\"` is cached\n- `\"Reset my password please\"` â†’ Cache miss (different string)\n\n**Semantic Caching (Step 2):** Uses embeddings to match similar questions\n- Both questions above â†’ Cache hit (same intent)\n\n> [!IMPORTANT]\n> **Production Pattern**: Use a **layered approach**:\n> 1. **Step 1: Exact Match** (Redis) - Fast, cheap, high precision\n> 2. **Step 2: Semantic Match** (Vector DB) - Slower, catches paraphrases  \n> 3. **Step 3: LLM Call** - Only if both caches miss\n\n## The Cost Impact\n\n**Without caching:**\n- 10,000 duplicate questions/day Ã— $0.03/call = **$300/day** = **$9,000/month**\n\n**With caching:**\n- 100 unique questions/day Ã— $0.03/call = **$3/day** = **$90/month**\n\n**Savings: $8,910/month** (99% cost reduction)\n\n## Production Semantic Caching\n\n### 1. Redis with TTL\n```python\nimport redis\nimport hashlib\n\ncache = redis.Redis()\n\ndef get_cached_answer(question, ttl=3600):\n    key = hashlib.sha256(question.encode()).hexdigest()\n    cached = cache.get(key)\n    \n    if cached:\n        return cached.decode()\n    \n    answer = call_openai_api(question)\n    cache.setex(key, ttl, answer)  # Expire after 1 hour\n    return answer\n```\n\n### 2. Semantic Similarity - Step 2 (Advanced)\nUse embeddings to match similar questions - catches paraphrases:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef semantic_cache_lookup(question, threshold=0.95):\n    # First check exact match cache (Step 1)\n    if question in exact_cache:\n        return exact_cache[question]\n    \n    # Step 2: Check semantic similarity\n    question_embedding = model.encode(question)\n    \n    # Check vector database for similar cached questions\n    results = vector_db.search(question_embedding, top_k=1)\n    \n    if results and results[0].similarity > threshold:\n        return results[0].cached_answer\n    \n    # Step 3: No match - call API and cache both ways\n    answer = call_openai_api(question)\n    exact_cache[question] = answer  # Exact cache\n    vector_db.insert(question_embedding, answer)  # Semantic cache\n    return answer\n```\n\n## When to Use Caching\n\nâœ… **Good for:**\n- FAQ-style questions\n- Static knowledge queries\n- High-volume endpoints\n\nâŒ **Bad for:**\n- Personalized responses\n- Real-time data queries\n- Time-sensitive information\n\n> [!IMPORTANT]\n> **Production Tip**: Always set TTL (expiration) on caches. Otherwise, you'll serve stale answers forever. For general knowledge, 24 hours is safe. For product info, use 1 hour.\n"
    },
    {
        "id": "ai-context-001",
        "title": "The Context Guillotine",
        "difficulty": "Medium",
        "summary": "Implement sliding window context management to prevent token limit crashes.",
        "description": "\n# The Scenario\n\nYour customer uploads a **50-page PDF** (10,000 tokens) to your chatbot.\n\nThey ask: *\"Summarize the conclusion.\"*\n\nYour code crashes:\n\n```\nOpenAI API Error: context_length_exceeded (8192 tokens)\n```\n\nThe LLM can't process 10,000 tokens. You hit the context limit.\n\n# The Problem\n\nYour code naively appends everything to history:\n\n```python\nhistory.append(user_input)  # ðŸ’¥ Grows forever\nllm.chat(history)\n```\n\nAfter a few exchanges with long documents, you exceed the 8k token limit and crash.\n\n# The Goal\n\nImplement a **Sliding Window** strategy:\n1. Always keep the System Prompt (first message)\n2. Always keep the Latest User Message\n3. Truncate the middle history to fit within token limits\n\n**Requirements:**\n- Target token limit: 4000 tokens\n- Must preserve system prompt\n- Must include latest user message\n- Truncate old messages from history\n\n> **Note**: In production, you'd use tiktoken for accurate counting, but we'll use a simple word-based approximation.\n",
        "brokenCode": "def build_context(system_prompt, chat_history, user_input, token_limit=4000):\n    \"\"\"\n    PROBLEM: Appends everything without checking token limits.\n    Crashes when total exceeds model's context window.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt}\n    ]\n    \n    # Blindly append all history\n    for msg in chat_history:\n        messages.append(msg)\n    \n    # Add new user input\n    messages.append({\"role\": \"user\", \"content\": user_input})\n    \n    return messages\n\ndef count_tokens(text):\n    \"\"\"Simple approximation: 1 token â‰ˆ 0.75 words\"\"\"\n    return len(text.split())\n",
        "testCode": "import pytest\n\ndef test_handles_large_context():\n    \"\"\"\n    Test that function truncates history when it exceeds token limit.\n    \"\"\"\n    from solution import build_context, count_tokens\n    \n    system_prompt = \"You are a helpful assistant.\"\n    \n    # Create a massive chat history (way over limit)\n    chat_history = [\n        {\"role\": \"user\", \"content\": \"Message \" * 1000},\n        {\"role\": \"assistant\", \"content\": \"Reply \" * 1000},\n        {\"role\": \"user\", \"content\": \"Question \" * 1000},\n        {\"role\": \"assistant\", \"content\": \"Answer \" * 1000},\n    ]\n    \n    user_input = \"Latest question \" * 100\n    \n    result = build_context(system_prompt, chat_history, user_input, token_limit=4000)\n    \n    # Calculate total tokens\n    total_tokens = sum(count_tokens(msg[\"content\"]) for msg in result)\n    \n    # Should be under limit\n    assert total_tokens <= 4000, f\"Context too large: {total_tokens} tokens (limit: 4000)\"\n    \n    # System prompt must be preserved\n    assert result[0][\"role\"] == \"system\"\n    assert result[0][\"content\"] == system_prompt\n    \n    # Latest user message must be included\n    assert result[-1][\"role\"] == \"user\"\n    assert result[-1][\"content\"] == user_input\n\ndef test_preserves_system_prompt():\n    \"\"\"\n    Verify system prompt is always kept, even when truncating.\n    \"\"\"\n    from solution import build_context\n    \n    system_prompt = \"Critical system instructions.\"\n    chat_history = [{\"role\": \"user\", \"content\": \"x \" * 5000}]\n    user_input = \"New question\"\n    \n    result = build_context(system_prompt, chat_history, user_input, token_limit=1000)\n    \n    # First message must be system prompt\n    assert result[0][\"role\"] == \"system\"\n    assert result[0][\"content\"] == system_prompt\n",
        "successMessage": "Context Managed! No more token limit crashes.",
        "debrief": "\n# Debrief: Sliding Window Context Management\n\n## Why Context Limits Exist\n\nLLMs have fixed context windows based on their architecture:\n\n| Model | Context Window |\n|-------|----------------|\n| GPT-3.5 Turbo | 4k / 16k tokens |\n| GPT-4 | 8k / 32k tokens |\n| GPT-4 Turbo | 128k tokens |\n| Claude 3 | 200k tokens |\n\nExceeding this limit causes **hard crashes**, not degraded performance.\n\n## The Sliding Window Strategy\n\nThe \"Lost in the Middle\" research shows that LLMs pay most attention to:\n1. **The beginning** (system prompt, instructions)\n2. **The end** (latest user query)\n\nMiddle content is often ignored or hallucinated.\n\n**Sliding Window** exploits this:\n- âœ… Keep system prompt (critical instructions)\n- âœ… Keep latest messages (current context)\n- âŒ Drop old messages (least important)\n\n## Production Token Counting\n\nOur simple word count is inaccurate. Use **tiktoken** in production:\n\n```python\nimport tiktoken\n\ndef count_tokens_accurate(text, model=\"gpt-4\"):\n    encoding = tiktoken.encoding_for_model(model)\n    return len(encoding.encode(text))\n\n# Example:\ncount_tokens_accurate(\"Hello world\")  # 2 tokens\n```\n\nDifferent models use different tokenizers!\n\n## Advanced Context Management\n\n### 1. Summarization\nSummarize old messages instead of deleting:\n\n```python\ndef summarize_old_context(chat_history):\n    old_messages = chat_history[:-5]  # All but last 5\n    summary = cheap_llm.summarize(old_messages)\n    return [{\"role\": \"system\", \"content\": f\"Previous context: {summary}\"}]\n```\n\n### 2. Vector Retrieval\nStore all messages in a vector DB, retrieve only relevant ones:\n\n```python\nrelevant_history = vector_db.search(user_query, top_k=3)\nmessages = [system_prompt] + relevant_history + [user_input]\n```\n\n### 3. Hierarchical Windowing\nKeep different recency levels:\n\n```python\ncontext = [\n    system_prompt,\n    *last_50_messages[-2:],   # Last 2 msgs: full detail\n    *last_50_messages[-10:-2].summarize(),  # 3-10: summarized\n    user_input\n]\n```\n\n## Real-World Impact\n\nA chatbot with 100 daily users, each doing 20 exchanges:\n\n| Strategy | Avg Tokens/Request | Cost/Day | Crashes |\n|----------|-------------------|----------|---------|\n| No limit | 12,000 | $120 | 50+ |\n| Sliding Window (4k) | 3,800 | $38 | 0 |\n\n**Savings: $82/day = $2,460/month**\n\n> [!IMPORTANT]\n> **Always count tokens BEFORE sending to the LLM**. Post-crash error handling is too late - you've already wasted time and frustrated users. Proactive truncation is the only safe approach.\n\n## When to Use Each Strategy\n\n| Strategy | Use Case | Cost | Complexity |\n|----------|----------|------|------------|\n| Sliding Window | Chat apps, simple bots | Free | Low |\n| Summarization | Long conversations | 2x | Medium |\n| Vector Retrieval | RAG, knowledge bases | 3x | High |\n"
    }
];

export const tracks: Track[] = [
    {
        "id": "systems-resilience",
        "title": "Systems Resilience",
        "description": "Master the failure modes of high-throughput distributed systems. Debug the logic that crashes production.",
        "challengeIds": [
            "redos-cpu-killer",
            "rate-limit-token-bucket-002"
        ]
    },
    {
        "id": "ai-architect",
        "title": "The AI Architect",
        "description": "Build the robust infrastructure that wraps LLMs. Master Semantic Caching, Context Windows, and Streaming stability.",
        "challengeIds": [
            "ai-cost-cache-002",
            "ai-context-001"
        ]
    }
];
