import { Challenge, Track } from '../types/challenge';

export const challenges: Challenge[] = [
    {
        "id": "redos-cpu-killer",
        "title": "The Regex Assassin",
        "difficulty": "Medium",
        "summary": "Fix a catastrophic regex backtracking vulnerability that freezes the production CPU.",
        "description": "\n# The Scenario\n\nYou are building a **data validation pipeline** that processes user-submitted strings before parsing them.\nTo prevent malicious input, you run a local Python validator to check patterns before further processing.\n\n# The Incident\n\nA user pasted a malformed pattern string into your API. Suddenly, your backend **froze**.\nNo errors, no crash logs—just 100% CPU usage.\nYour downstream services are sitting idle, but your validation container is completely unresponsive.\n\n# The Stakes\n\n- **The Victim**: Your validation service.\n- **The Culprit**: A regex validator intended to check for repeating character patterns.\n- **The Result**: A \"ReDoS\" (Regular Expression Denial of Service) that forces your CPU to check millions of paths for a single string.\n\n# The Goal\n\nPatch the `validate_pattern` function.\n- The current logic uses `^((A+)+)+$` which has **O(2^n)** complexity.\n- **Your Task**: Rewrite the regex to accept valid patterns instantly (under 10ms) without hanging on malformed inputs.",
        "brokenCode": "import re\n\ndef validate_pattern(sequence):\n    # CONTEXT: We need to verify the input is a valid repeating pattern\n    # (a sequence consisting ONLY of the character 'A').\n    \n    # This pattern causes catastrophic backtracking.\n    # It attempts to match \"one or more As\", repeated, inside a repeating group.\n    pattern = r\"^((A+)+)+$\"\n    \n    if re.match(pattern, sequence):\n        return True\n    return False\n",
        "testCode": "import pytest\nimport multiprocessing\nimport time\nimport sys\n\ndef run_validation(sequence, return_dict):\n    try:\n        from solution import validate_pattern\n        start = time.time()\n        result = validate_pattern(sequence)\n        end = time.time()\n        return_dict['result'] = result\n        return_dict['duration'] = (end - start) * 1000 # ms\n    except Exception as e:\n        return_dict['error'] = str(e)\n\ndef test_catastrophic_backtracking():\n    \"\"\"\n    The Attack: 25 'A's followed by a Cytosine ('C').\n    The regex engine will try every combination of nested groups before failing.\n    We enforce a strict 1.0s timeout to simulate the 'Hang'.\n    \"\"\"\n    # 25 As followed by one C (which breaks the pattern)\n    attack_string = \"A\" * 25 + \"C\"\n    \n    manager = multiprocessing.Manager()\n    return_dict = manager.dict()\n    \n    p = multiprocessing.Process(target=run_validation, args=(attack_string, return_dict))\n    p.start()\n    \n    # Wait for 1.0 second max\n    p.join(timeout=1.0)\n    \n    if p.is_alive():\n        p.terminate()\n        p.join()\n        print(\"\\n[KERNEL] [1234.567890] watchdog: BUG: soft lockup - CPU#0 stuck for 22s! [python3:1337]\")\n        print(\"[WATCHDOG] Process stuck for 1000ms. Killing...\")\n        pytest.fail(\"❌ CATASTROPHIC BACKTRACKING DETECTED: The pattern validator hung the CPU! (Timeout > 1.0s)\")\n    \n    if 'error' in return_dict:\n        pytest.fail(f\"Runtime Error: {return_dict['error']}\")\n        \n    duration = return_dict.get('duration', 0)\n    result = return_dict.get('result')\n    \n    # Success Criteria\n    assert result is False, \"The validator should reject the malformed sequence containing 'C'.\"\n    \n    # CALIBRATION: Measure baseline performance of the environment\n    start_cal = time.time()\n    for _ in range(100000): pass\n    end_cal = time.time()\n    calibration_time = (end_cal - start_cal) * 1000 # ms\n    \n    # Expected baseline on fast machine is ~3-5ms. If slower, scale the limit.\n    base_limit = 10.0\n    expected_calibration = 5.0 \n    \n    scaling_factor = max(1.0, calibration_time / expected_calibration)\n    adjusted_limit = base_limit * scaling_factor\n    \n    # Add a small buffer for noise\n    adjusted_limit += 5.0\n    \n    print(f\"\\n[DEBUG] Calibration: {calibration_time:.2f}ms | Scaling: {scaling_factor:.2f}x | Limit: {adjusted_limit:.2f}ms\")\n\n    if duration < adjusted_limit:\n        print(f\"[WAF] [ACCESS] 192.168.1.50 - - [02/Dec/2025:10:00:00 +0000] \\\"POST /api/validate HTTP/1.1\\\" 200 45 \\\"-\\\" \\\"Python/3.9\\\" (Time: {duration:.2f}ms)\")\n    else:\n        print(f\"[WAF] [WARN] Slow request detected: {duration:.2f}ms\")\n\n    assert duration < adjusted_limit, f\"Too slow! Took {duration:.2f}ms. Target < {adjusted_limit:.2f}ms (Adjusted for env speed).\"\n\ndef test_valid_input():\n    \"\"\"Ensure we didn't break valid pattern inputs.\"\"\"\n    from solution import validate_pattern\n    assert validate_pattern(\"AAAAAAAAAAAAAAAAAAAAAAAAA\") == True\n    assert validate_pattern(\"AAAAA\") == True\n",
        "successMessage": "Pattern Validated! Your validation service is now safe from ReDoS.",
        "debrief": "\n# Debrief: The ReDoS Trap in Input Validation\n\n## Why this matters\nYou were \"validating user input patterns\" before processing them.\nIn this challenge, we simulated a **repeating character validation**, but the principle applies to any pattern matching (e.g., email validation, phone numbers).\n\n## The Mechanics: O(2^n) Complexity\nThe regex `^((A+)+)+$` fails because of **Nested Quantifiers**.\nWhen the validator meets the character 'C' at the end of the string, it doesn't fail immediately. It backtracks and tries to regroup the previous 25 'A's in every possible combination (2^25 attempts) to see if *that* works.\n\n## The Impact on Production\nIn a production system:\n1.  **The Choke Point:** Regex validation runs *before* downstream processing.\n2.  **The Hang:** If this regex hangs, the application stops responding.\n3.  **The Cost:** Your hosting provider (AWS/GCP/Vercel) is still charging you for the CPU time while it spins at 100%.\n\n## Event Loop Blocking in Async Python\nEven if your application uses `async/await`, **regex operations are synchronous** and CPU-bound.\nWhen a ReDoS attack occurs:\n- The regex engine blocks the event loop\n- All other async tasks are starved\n- Your entire service becomes unresponsive, even for simple health checks\n\nTo prevent this in async systems:\n```python\n# Run CPU-intensive regex in a thread pool\nloop = asyncio.get_event_loop()\nresult = await loop.run_in_executor(None, validate_pattern, user_input)\n```\n\nOr better yet, use non-backtracking engines like `re2`.\n\n## The Fix\nSimplify.\n* **Bad:** `^((A+)+)+$`\n* **Good:** `^A+$`\n* **Better:** Use non-backtracking regex engines (like `re2`) or length limits for critical validation paths.\n"
    },
    {
        "id": "rate-limit-token-bucket-002",
        "title": "The Burst Handler",
        "difficulty": "Medium",
        "summary": "Implement a Token Bucket algorithm to allow traffic bursts while respecting long-term rate limits.",
        "description": "\n# The Scenario\n\nYou are scraping a strict API. It allows bursts (up to 10 requests instantly) but enforces a long-term rate of 1 request/second.\n\n# The Problem\n\nYour current implementation uses a `StrictLimiter`. It enforces a rigid 1-second gap between *every* request (Leaky Bucket style).\n\nWhile this is \"safe\" (you won't get banned), it is **too slow**. It fails to utilize the API's \"Burst\" allowance. If you have 10 requests to make, you should be able to make them *instantly* because you haven't used your allowance yet. The `StrictLimiter` forces you to wait 10 seconds.\n\n# The Goal\n\nRefactor the `StrictLimiter` class into a **Token Bucket** algorithm.\n- **Bank Tokens**: Accumulate tokens when idle (up to capacity 10).\n- **Burst**: Spend tokens instantly to make requests without waiting.\n- **Throttle**: Only wait when the bucket is empty.\n",
        "brokenCode": "import time\nimport requests\n\nclass StrictLimiter:\n    def __init__(self, rate):\n        self.rate = rate\n        # Strict spacing: 1 request every 1/rate seconds\n        self.interval = 1.0 / rate\n        self.last_call = 0.0\n\n    def wait(self):\n        # Good use of monotonic time, but bad logic for bursts!\n        now = time.monotonic()\n        elapsed = now - self.last_call\n        if elapsed < self.interval:\n            time.sleep(self.interval - elapsed)\n        self.last_call = time.monotonic()\n\n# TODO: Refactor this into a TokenBucket class!\n# class TokenBucket:\n#     def __init__(self, rate, capacity):\n#         ...\n#     def consume(self):\n#         ...\n\ndef fetch_data(url):\n    # This limiter is safe but too slow!\n    limiter = StrictLimiter(rate=1.0)\n    \n    # Simulate 10 requests\n    for _ in range(10):\n        limiter.wait()\n        response = requests.get(url)\n    \n    return \"Done\"\n",
        "testCode": "import pytest\nfrom unittest.mock import MagicMock, patch\nimport time\n\ndef test_burst_capability():\n    \"\"\"\n    Goal: Verify that the implementation allows a burst of 10 requests instantly.\n    The StrictLimiter fails this because it sleeps between every request.\n    \"\"\"\n    try:\n        from solution import TokenBucket\n    except ImportError:\n        pytest.fail(\"Could not import 'TokenBucket'. Did you refactor the class?\")\n\n    # We mock both time and monotonic to ensure the user can use either (though monotonic is preferred)\n    with patch('time.time') as mock_time, patch('time.monotonic') as mock_mono, patch('time.sleep') as mock_sleep:\n        # Shared time state\n        current_time = [1000.0] \n        \n        # Both clocks return the same flow of time for simplicity in tests\n        mock_time.side_effect = lambda: current_time[0]\n        mock_mono.side_effect = lambda: current_time[0]\n        \n        def sleep_side_effect(seconds):\n            current_time[0] += seconds\n        mock_sleep.side_effect = sleep_side_effect\n\n        # Initialize: Rate 1/sec, Capacity 10\n        bucket = TokenBucket(rate=1.0, capacity=10)\n\n        # BURST TEST: Consume 10 tokens\n        start_time = current_time[0]\n        for i in range(10):\n            bucket.consume()\n            print(f\"[GATEWAY] [INFO] Request forwarded to backend. TokenBucket: {10 - i - 1}/10. Latency: 0.1ms\")\n        end_time = current_time[0]\n\n        duration = end_time - start_time\n        \n        # StrictLimiter would take ~9-10 seconds here.\n        # TokenBucket should take 0 seconds (logic time).\n        assert duration < 0.1, f\"Too slow! 10 requests took {duration}s. Expected burst (~0s sleep).\"\n\ndef test_rate_limit_enforcement():\n    \"\"\"\n    Goal: Verify that after the burst, the rate limit (1/sec) is enforced.\n    \"\"\"\n    from solution import TokenBucket\n    \n    with patch('time.time') as mock_time, patch('time.monotonic') as mock_mono, patch('time.sleep') as mock_sleep:\n        current_time = [1000.0]\n        mock_time.side_effect = lambda: current_time[0]\n        mock_mono.side_effect = lambda: current_time[0]\n        \n        def sleep_side_effect(seconds):\n            current_time[0] += seconds\n        mock_sleep.side_effect = sleep_side_effect\n\n        bucket = TokenBucket(rate=1.0, capacity=10)\n\n        # Drain the bucket (10 tokens)\n        for _ in range(10):\n            bucket.consume()\n            \n        # Now bucket is empty. Next request MUST sleep ~1.0s\n        mock_sleep.reset_mock()\n        print(\"\\n[GATEWAY] [WARN] Rate Limit Exceeded for IP 10.0.0.1. Bucket Empty. Throttling...\")\n        bucket.consume()\n        \n        assert mock_sleep.called, \"You exceeded the rate limit! Should have slept when bucket was empty.\"\n        sleep_time = mock_sleep.call_args[0][0]\n        assert 0.9 <= sleep_time <= 1.1, f\"Should sleep ~1.0s, slept {sleep_time}s\"\n        print(f\"[GATEWAY] [INFO] Request allowed after throttle. Waited {sleep_time:.2f}s.\")\n",
        "successMessage": "Bucket Full! You've mastered the Token Bucket algorithm.",
        "debrief": "\n# Debrief: Traffic Shaping vs Rate Limiting\n\n## Mechanics\nThe Token Bucket algorithm is based on a simple refill formula:\n```python\n# Use monotonic time for durations!\nnew_tokens = (now - last_refill) * rate\n```\n\nWe cap the tokens at `capacity` so you can't accumulate infinite burst allowance.\n\n## Visualizing the Bucket\n```text\n       (Rate: 1 token/sec)\n              |\n              v\n      [  Token Drop  ]\n              |\n      +---------------+\n      |  T  T  T  T   |  <- Capacity (10)\n      |  T  T  T  T   |\n      +---------------+\n              |\n      [   Consumer    ]  <- Bursts drain tokens instantly\n              |\n              v\n       (Allowed Request)\n```\n\n## Traffic Shaping vs Rate Limiting\n\n### Rate Limiting (Token Bucket)\n- **Purpose**: Prevent abuse by limiting total throughput over time\n- **Behavior**: Allows bursts, then throttles\n- **Use Case**: API rate limits, scraping, preventing abuse\n- **Implementation**: Token Bucket (this challenge) or Sliding Window\n\n### Traffic Shaping (Leaky Bucket)\n- **Purpose**: Smooth out traffic to prevent jitter and maintain steady flow\n- **Behavior**: Rigid spacing between requests, no bursts\n- **Use Case**: Network QoS, video streaming, preventing congestion\n- **Implementation**: Leaky Bucket (strict spacing)\n\n**Key Difference**: Rate limiting cares about *total volume*. Traffic shaping cares about *consistent pacing*.\n\nIn production:\n- Use **Token Bucket** for user-facing APIs (allow bursts for good UX)\n- Use **Leaky Bucket** for backend-to-backend communication (prevent overwhelming downstream services)\n\n## Senior Engineer Notes\n> [!WARNING]\n> **Time is Hard**: Always use `time.monotonic()` for measuring durations. `time.time()` can jump backwards or forwards if the server syncs with NTP, which will break your rate limiter logic in production.\n\n> [!IMPORTANT]\n> **Distributed Systems**: This code only works for a single process. In a distributed system (e.g., Celery workers, Kubernetes), you must use a shared store like **Redis** (using `INCR` and `EXPIRE` or Lua scripts) to maintain the bucket state across all workers.\n"
    },
    {
        "id": "ai-cost-cache-002",
        "title": "The Wallet Burner",
        "difficulty": "Medium",
        "summary": "Implement semantic caching to reduce AI API costs by detecting similar queries.",
        "description": "\n# The Scenario\n\nYour AI startup is burning **$5,000/month** on OpenAI bills.\nUsers ask the same questions in slightly different ways:\n- \"What is your pricing?\"\n- \"How much does it cost?\"\n- \"Price list please\"\n\nExact match caching fails here. You pay for all 3 queries.\n\n# The Goal\n\nImplement **Semantic Caching** using vector similarity.\n1. Convert the user query to an embedding (vector).\n2. Compare it with cached query embeddings.\n3. If `similarity > 0.9`, return the cached response.\n\n**Helpers Provided:**\n- `mock_embed(text)`: Returns a list of floats (vector).\n- `cosine_similarity(v1, v2)`: Returns a float between 0.0 and 1.0.\n\n**Requirements:**\n- Iterate through the cache.\n- Find the cached query with the highest similarity.\n- If similarity > 0.9, return cached response.\n- Otherwise, call LLM and cache the new query/response.\n",
        "brokenCode": "def get_ai_response(user_query, mock_llm, cache):\n    \"\"\"\n    PROBLEM: Only checks exact matches.\n    Fails to catch \"How much?\" vs \"Price?\"\n    \"\"\"\n    # Naive Exact Match\n    if user_query in cache:\n        return cache[user_query]['response']\n        \n    # Call LLM\n    response = mock_llm.generate(user_query)\n    \n    # Store in cache\n    cache[user_query] = {\n        'response': response,\n        'embedding': [0.1, 0.2] # Mock embedding\n    }\n    return response\n",
        "testCode": "import pytest\nfrom unittest.mock import MagicMock\n\ndef test_semantic_hit():\n    \"\"\"\n    Verify that semantically similar queries hit the cache.\n    \"\"\"\n    from solution import get_ai_response\n    \n    # Mock Helpers\n    mock_llm = MagicMock()\n    mock_llm.generate.return_value = \"Pricing is $10.\"\n    \n    # Pre-filled cache with \"What is the price?\"\n    cache = {\n        \"What is the price?\": {\n            \"response\": \"Pricing is $10.\",\n            \"embedding\": [1.0, 0.0] # Simplified vector\n        }\n    }\n    \n    # User asks \"How much cost?\" (Similar vector)\n    # We mock the embedding function to return a similar vector\n    # Cosine sim of [1,0] and [0.95, 0.3] is high\n    \n    # In the solution, they should use the provided helpers.\n    # We'll inject them or mock them if they are imported.\n    # For this test, we assume the solution defines/imports them.\n    \n    response = get_ai_response(\"How much cost?\", mock_llm, cache)\n    \n    assert response == \"Pricing is $10.\"\n    # CRITICAL: Should NOT call LLM\n    assert mock_llm.generate.call_count == 0, \"Semantic match failed! LLM was called.\"\n    print(f\"[CACHE] [HIT] Semantic match found (Similarity: 0.95). Cost: $0.00\")\n\ndef test_cache_miss_updates_cache():\n    \"\"\"\n    Verify that new queries are added to the cache.\n    \"\"\"\n    from solution import get_ai_response\n    mock_llm = MagicMock()\n    mock_llm.generate.return_value = \"New Answer\"\n    cache = {}\n    \n    get_ai_response(\"New Query\", mock_llm, cache)\n    \n    assert len(cache) == 1\n    assert \"New Query\" in cache\n    assert mock_llm.generate.call_count == 1\n",
        "successMessage": "Cache Hit! You're saving money on every similar query.",
        "debrief": "\n# Debrief: Semantic Caching\n\n## Beyond Exact Match\nExact match caching (Redis keys) only catches 5-10% of duplicates.\nSemantic caching catches 40-60% by understanding **intent**.\n\n## How it Works\n1. **Embed**: Convert query to vector (e.g., OpenAI `text-embedding-3-small`).\n2. **Search**: Find nearest neighbors in vector DB (Pinecone/Redis).\n3. **Threshold**: If distance < 0.1, it's the same question.\n\n## The Risk: False Positives\nIf threshold is too low (e.g., 0.7), you might return wrong answers.\n- Q: \"Can I delete my account?\"\n- Cached: \"Can I delete my post?\"\n- Similarity: 0.85 -> **Wrong Answer!**\n\n**Fix**: Use a high threshold (0.95) or use a \"Reranker\" to verify the match.\n"
    },
    {
        "id": "ai-context-001",
        "title": "The Context Guillotine",
        "difficulty": "Hard",
        "summary": "Implement a summarization strategy to preserve context when history exceeds token limits.",
        "description": "\n# The Scenario\n\nYour chatbot has a **4000 token limit**.\nUsers complain that the bot \"forgets\" things mentioned 20 messages ago.\nCurrently, you just **truncate** old messages.\n\n# The Problem\n\nTruncation deletes information.\n- User: \"My name is Alice.\" (Message #1)\n- ... 50 messages later ...\n- User: \"What is my name?\"\n- Bot: \"I don't know.\" (Message #1 was deleted)\n\n# The Goal\n\nImplement **Summarization**:\n1. When history exceeds the limit, don't just delete.\n2. Call `mock_summarize(text)` on the oldest messages.\n3. Replace them with a single \"System Summary\" message.\n\n**Requirements:**\n- Target limit: 4000 tokens.\n- If history > limit:\n    - Identify oldest messages to drop.\n    - Summarize them.\n    - Insert summary at the start of history.\n- Preserve System Prompt and Latest User Message.\n",
        "brokenCode": "def build_context(system_prompt, chat_history, user_input, token_limit=4000):\n    \"\"\"\n    PROBLEM: Truncates history, losing critical info.\n    \"\"\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n    \n    # Naive truncation\n    # If history is too long, we just take the last N messages\n    # We lose everything before that.\n    \n    # ... (Implementation that just slices list)\n    \n    messages.extend(chat_history[-5:]) # Keep last 5 only\n    messages.append({\"role\": \"user\", \"content\": user_input})\n    \n    return messages\n\ndef mock_summarize(text):\n    return f\"Summary of: {text[:20]}...\"\n",
        "testCode": "import pytest\n\ndef test_summarizes_old_context():\n    \"\"\"\n    Verify that old context is summarized, not deleted.\n    \"\"\"\n    from solution import build_context\n    \n    system_prompt = \"You are a helpful assistant.\"\n    # History with critical info at the start\n    chat_history = [\n        {\"role\": \"user\", \"content\": \"My name is Alice.\"},\n        {\"role\": \"assistant\", \"content\": \"Hello Alice.\"},\n        # ... filler messages that will exceed token limit ...\n    ] + [{\"role\": \"user\", \"content\": \"This is filler message number \" + str(i) + \" with extra padding to ensure we exceed tokens.\"} for i in range(100)]\n    \n    user_input = \"What is my name?\"\n    \n    # This should trigger summarization\n    result = build_context(system_prompt, chat_history, user_input, token_limit=1000)\n    \n    # 1. Check for summary message\n    # Should be after system prompt\n    summary_msg = result[1]\n    assert summary_msg[\"role\"] == \"system\"\n    assert \"summary\" in summary_msg[\"content\"].lower()\n    \n    print(f\"[MEMORY] [INFO] Old context compressed into summary: '{summary_msg['content']}'\")\n\ndef test_preserves_recent_context():\n    \"\"\"\n    Verify recent messages are kept as-is.\n    \"\"\"\n    from solution import build_context\n    \n    chat_history = [{\"role\": \"user\", \"content\": f\"Msg {i}\"} for i in range(10)]\n    user_input = \"New\"\n    \n    result = build_context(\"Sys\", chat_history, user_input, token_limit=5000)\n    \n    # Should keep all if under limit\n    assert len(result) == 12 # Sys + 10 hist + User\n",
        "successMessage": "Memory Upgraded! Your bot now remembers everything (compressed).",
        "debrief": "\n# Debrief: Infinite Memory (Sort of)\n\n## The Trade-off\n- **Truncation**: Perfect recall of recent events. Zero recall of past.\n- **Summarization**: Fuzzy recall of past. Perfect recall of recent.\n\n## Recursive Summarization\nIn production, as the conversation grows, you summarize the summaries.\n1. `Summary 1` + `New Messages` -> `Summary 2`\n2. `Summary 2` + `New Messages` -> `Summary 3`\n\nThis creates a \"rolling state\" that persists indefinitely.\n\n## LangChain Memory\nLangChain's `ConversationSummaryBufferMemory` implements exactly this pattern:\n- Keep a buffer of recent messages (window).\n- Compile older messages into a summary.\n"
    },
    {
        "id": "obs-tracing-001",
        "title": "The Silent Failure",
        "difficulty": "Medium",
        "summary": "Implement distributed tracing to find the root cause of a failure in a microservice chain.",
        "description": "\n# The Scenario\n\nUser reports: \"I clicked 'Generate' and nothing happened.\"\nYour API logs show: `200 OK`.\nThe error happened deep in a microservice chain: `API -> Orchestrator -> VectorDB -> LLM`.\n\nWithout tracing, you have no idea which service failed or why.\n\n# The Problem\n\nYour services swallow exceptions and return generic messages.\n```python\ntry:\n    call_downstream()\nexcept Exception:\n    return None  # The error is lost forever\n```\n\n# The Goal\n\nImplement **Trace ID Propagation**:\n1. Accept a `trace_id` from the upstream caller.\n2. Log the `trace_id` with every error message.\n3. Pass the `trace_id` to the downstream service.\n\n**Requirements:**\n- Function `process_request(trace_id)`\n- Log errors using `print(f\"[ERROR] [{trace_id}] ...\")`\n- Pass `trace_id` to `call_service_b(trace_id)`\n",
        "brokenCode": "def process_request(trace_id):\n    \"\"\"\n    PROBLEM: Swallows errors and breaks the trace chain.\n    \"\"\"\n    print(\"Processing request...\")\n    \n    try:\n        # Call downstream service\n        # BUG: Forgot to pass trace_id!\n        call_service_b() \n    except Exception as e:\n        # BUG: Log doesn't include trace_id\n        print(f\"Something went wrong: {e}\")\n        return \"Error\"\n\ndef call_service_b(trace_id=None):\n    # Simulate a failure deep in the stack\n    raise ValueError(\"Connection refused\")\n",
        "testCode": "import pytest\nfrom unittest.mock import patch\nimport io\nimport sys\n\ndef test_propagates_trace_id():\n    \"\"\"\n    Verify that trace_id is passed to downstream service.\n    \"\"\"\n    from solution import process_request, call_service_b\n    \n    trace_id = \"abc-123\"\n    \n    # Capture stdout\n    captured_output = io.StringIO()\n    sys.stdout = captured_output\n    \n    # Mock call_service_b to verify arguments\n    with patch('solution.call_service_b') as mock_b:\n        mock_b.side_effect = ValueError(\"Deep failure\")\n        \n        process_request(trace_id)\n        \n        # Check if trace_id was passed\n        mock_b.assert_called_with(trace_id)\n        \n    sys.stdout = sys.__stdout__\n    print(f\"[TRACE] [PASS] Trace ID '{trace_id}' propagated to Service B.\")\n\ndef test_logs_trace_id_on_error():\n    \"\"\"\n    Verify that the error log contains the trace_id.\n    \"\"\"\n    from solution import process_request\n    \n    trace_id = \"debug-999\"\n    \n    # Capture stdout\n    captured_output = io.StringIO()\n    sys.stdout = captured_output\n    \n    process_request(trace_id)\n    \n    output = captured_output.getvalue()\n    sys.stdout = sys.__stdout__\n    \n    # Check for trace_id in logs\n    assert trace_id in output, \"Log missing trace_id!\"\n    assert \"ERROR\" in output or \"Error\" in output, \"Log missing error level\"\n    \n    print(f\"[LOGS] [PASS] Found trace_id '{trace_id}' in error logs.\")\n",
        "successMessage": "Trace Found! You can now follow a request across the entire stack.",
        "debrief": "\n# Debrief: Distributed Tracing\n\n## The \"Needle in a Haystack\"\nIn microservices, a single user request triggers 50+ internal RPC calls.\nIf one fails, you need to find *which one*.\n\n**Trace ID**: A unique string (UUID) generated at the API Gateway.\n**Span ID**: A unique ID for each individual operation (e.g., \"SQL Query\", \"HTTP Call\").\n\n## OpenTelemetry (OTel)\nIn production, don't implement this manually. Use **OpenTelemetry**.\n\n```python\nfrom opentelemetry import trace\n\ntracer = trace.get_tracer(__name__)\n\nwith tracer.start_as_current_span(\"process_request\") as span:\n    span.set_attribute(\"user.id\", \"123\")\n    call_downstream()\n```\n\nOTel automatically propagates headers (`traceparent`) over HTTP/gRPC.\n"
    },
    {
        "id": "obs-metrics-alert",
        "title": "The Metric Spike",
        "difficulty": "Medium",
        "summary": "Implement a sliding window error rate calculation to trigger alerts.",
        "description": "\n# The Scenario\n\nYour LLM provider started timing out 5% of requests.\nYou didn't notice for 3 days because you only alert if **>50%** of requests fail.\nYour users are churning.\n\n# The Goal\n\nImplement a **Sliding Window Error Rate**:\n1. Track the last 100 requests.\n2. Calculate `error_rate = errors / total`.\n3. If `error_rate > 0.05` (5%), return `True` (Trigger Alert).\n\n**Requirements:**\n- Use a list to store request status (1=Error, 0=Success).\n- Keep only the last 100 requests.\n- Return `True` if average > 0.05.\n",
        "brokenCode": "class ErrorMonitor:\n    def __init__(self):\n        self.errors = 0\n        self.total = 0\n        \n    def log_request(self, is_error):\n        \"\"\"\n        PROBLEM: Calculates global average since startup.\n        If you had 1M successsful requests yesterday, \n        1000 errors today won't trigger the alert.\n        \"\"\"\n        self.total += 1\n        if is_error:\n            self.errors += 1\n            \n    def should_alert(self):\n        if self.total == 0: return False\n        return (self.errors / self.total) > 0.05\n",
        "testCode": "import pytest\n\ndef test_sliding_window_alert():\n    \"\"\"\n    Verify that recent errors trigger the alert, even if history is good.\n    \"\"\"\n    from solution import ErrorMonitor\n    \n    monitor = ErrorMonitor()\n    \n    # 1. Simulate 1000 successful requests (History)\n    for _ in range(1000):\n        monitor.log_request(is_error=False)\n        \n    assert monitor.should_alert() == False\n    \n    # 2. Simulate spike: 10 errors in a row\n    # Global average: 10 / 1010 = 0.009 (No Alert)\n    # Sliding window (last 100): 10 / 100 = 0.10 (ALERT!)\n    for _ in range(10):\n        monitor.log_request(is_error=True)\n        \n    # Should alert now\n    if monitor.should_alert():\n        print(\"[ALERT] [PASS] Alert triggered correctly on recent spike.\")\n    else:\n        pytest.fail(\"Alert NOT triggered! You are likely using global average instead of sliding window.\")\n\ndef test_window_size():\n    \"\"\"\n    Verify window size is capped at 100.\n    \"\"\"\n    from solution import ErrorMonitor\n    monitor = ErrorMonitor()\n    \n    # Fill with errors\n    for _ in range(100):\n        monitor.log_request(True)\n    assert monitor.should_alert() == True\n    \n    # Flush with success\n    for _ in range(100):\n        monitor.log_request(False)\n    assert monitor.should_alert() == False\n",
        "successMessage": "Alert Triggered! You caught the outage in seconds, not days.",
        "debrief": "\n# Debrief: Metrics & Alerting\n\n## The \"Global Average\" Fallacy\nAverages hide spikes. If your service has 99.99% uptime over a year, but is 100% down right now, the **global average** is still 99.99%.\nYou need **Windowed Metrics** (last 1m, 5m, 1h).\n\n## Golden Signals\nGoogle SRE defines 4 golden signals to monitor:\n1. **Latency**: Time to serve a request.\n2. **Traffic**: Demand on the system (req/sec).\n3. **Errors**: Rate of failed requests.\n4. **Saturation**: How \"full\" the service is (CPU/RAM).\n\n## Tools\n- **Prometheus**: Scrapes metrics.\n- **Grafana**: Visualizes them.\n- **PagerDuty**: Wakes you up when the alert fires.\n"
    }
];

export const tracks: Track[] = [
    {
        "id": "systems-resilience",
        "title": "Systems Resilience",
        "description": "Master the fundamentals of distributed systems. Learn to handle timeouts, rate limits, and connection pools.",
        "challengeIds": [
            "redos-cpu-killer",
            "rate-limit-token-bucket-002"
        ]
    },
    {
        "id": "ai-architect",
        "title": "The AI Architect",
        "description": "Build the robust infrastructure that wraps LLMs. Master Semantic Caching, Context Windows, and Streaming stability.",
        "challengeIds": [
            "ai-cost-cache-002",
            "ai-context-001"
        ]
    },
    {
        "id": "observability",
        "title": "Observability & Debugging",
        "description": "Learn the essential tools to debug distributed AI systems. Practice Tracing, Metrics, and Structured Logging.",
        "challengeIds": [
            "obs-tracing-001",
            "obs-metrics-alert"
        ]
    }
];
